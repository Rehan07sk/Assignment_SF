This project implements an end‑to‑end Snowpipe pipeline to automatically load Parquet files from Amazon S3 into Snowflake. The setup creates dedicated Snowflake objects (role, warehouse, database, schema, file format, external stage, and pipe) along with an AWS S3 bucket and IAM role linked via a Snowflake storage integration. New Parquet files arriving in a specific S3 prefix trigger S3 event notifications to a Snowflake‑managed SQS queue, which in turn invokes Snowpipe to run a COPY INTO and continuously ingest data into the target Snowflake table.
